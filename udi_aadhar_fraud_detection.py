# -*- coding: utf-8 -*-
"""udi_Aadhar_fraud_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18aYobt60iLKq80GVhNl6Ik8geYwDJoF_
"""

# ============================================================================
# PART 1: INSTALLATION & IMPORTS
# ============================================================================

print("üöÄ Installing required libraries...")
!pip install -q imbalanced-learn xgboost catboost lightgbm optuna shap openpyxl xlrd

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
import os
warnings.filterwarnings('ignore')

# ML Libraries
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score,
                             f1_score, accuracy_score, precision_score, recall_score, roc_curve, auc)

import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostClassifier
from imblearn.over_sampling import SMOTE
import optuna
from optuna.samplers import TPESampler
import shap
import joblib
import pickle

RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

print("‚úÖ All libraries imported successfully!\n")

# ============================================================================
# PART 2: LOAD REAL AADHAAR DATASETS
# ============================================================================

def load_real_aadhaar_data():
    """Load the 3 actual Aadhaar CSV files"""
    print("="*80)
    print("üìÇ LOADING REAL AADHAAR DATASETS")
    print("="*80)

    datasets = {}

    # Try to load each dataset
    files = {
        'demographic': 'data_aadhaar_demographic.csv',
        'biometric': 'data_aadhaar_biometric.csv',
        'enrolment': 'data_aadhaar_enrolment.csv'
    }

    for name, filepath in files.items():
        try:
            df = pd.read_csv(filepath, low_memory=False)
            datasets[name] = df
            print(f"\n‚úÖ {name.upper()} loaded: {len(df):,} rows, {len(df.columns)} columns")
            print(f"   Columns: {list(df.columns[:5])}..." if len(df.columns) > 5 else f"   Columns: {list(df.columns)}")
        except FileNotFoundError:
            print(f"\n‚ö†Ô∏è  {filepath} not found - will use synthetic data")
            datasets[name] = None
        except Exception as e:
            print(f"\n‚ùå Error loading {filepath}: {e}")
            datasets[name] = None

    # If no real data found, generate synthetic
    if all(v is None for v in datasets.values()):
        print("\n‚ö†Ô∏è  No real data files found. Generating synthetic data for demonstration...")
        return None

    return datasets

datasets = load_real_aadhaar_data()

# ============================================================================
# PART 3: DATA PREPROCESSING & INTEGRATION
# ============================================================================

def preprocess_and_merge_data(datasets):
    """Preprocess and merge all 3 datasets"""
    print("\n" + "="*80)
    print("üîÑ PREPROCESSING & MERGING DATASETS")
    print("="*80)

    if datasets is None:
        # Generate synthetic data if no real files
        print("\nGenerating synthetic dataset...")
        return generate_synthetic_data()

    # Standardize column names
    def clean_columns(df):
        if df is None:
            return None
        df = df.copy()
        df.columns = df.columns.str.lower().str.strip().str.replace(' ', '_').str.replace('-', '_')
        return df

    df_demo = clean_columns(datasets.get('demographic'))
    df_bio = clean_columns(datasets.get('biometric'))
    df_enrol = clean_columns(datasets.get('enrolment'))

    # Identify date columns
    def find_date_column(df):
        if df is None:
            return None
        date_keywords = ['date', 'time', 'day', 'month', 'year']
        for col in df.columns:
            if any(kw in col.lower() for kw in date_keywords):
                try:
                    df[col] = pd.to_datetime(df[col], errors='coerce')
                    return col
                except:
                    continue
        return None

    # Identify location columns
    def find_location_columns(df):
        if df is None:
            return {}
        loc_cols = {}
        for col in df.columns:
            col_lower = col.lower()
            if 'state' in col_lower and 'state' not in loc_cols:
                loc_cols['state'] = col
            elif 'district' in col_lower and 'district' not in loc_cols:
                loc_cols['district'] = col
            elif any(x in col_lower for x in ['pin', 'postal', 'zip']) and 'pin' not in loc_cols:
                loc_cols['pin'] = col
        return loc_cols

    # Process demographic data (PRIMARY)
    if df_demo is not None:
        print("\nüìä Processing DEMOGRAPHIC data...")

        # Find key columns
        date_col = find_date_column(df_demo)
        loc_cols = find_location_columns(df_demo)

        print(f"   Date column: {date_col}")
        print(f"   Location columns: {loc_cols}")

        # Aggregate by date and location
        if date_col and loc_cols:
            group_cols = [date_col] + list(loc_cols.values())
            group_cols = [c for c in group_cols if c in df_demo.columns]

            # Aggregate numeric columns
            numeric_cols = df_demo.select_dtypes(include=[np.number]).columns.tolist()
            agg_dict = {col: 'sum' for col in numeric_cols if col not in group_cols}

            if agg_dict:
                df_combined = df_demo.groupby(group_cols).agg(agg_dict).reset_index()
                print(f"   ‚úÖ Aggregated to {len(df_combined):,} location-date groups")
            else:
                df_combined = df_demo.copy()
        else:
            df_combined = df_demo.copy()
            print("   ‚ö†Ô∏è  Could not identify date/location columns, using raw data")
    else:
        print("\n‚ö†Ô∏è  No demographic data - generating synthetic...")
        return generate_synthetic_data()

    # Merge biometric data
    if df_bio is not None:
        print("\nüìä Processing BIOMETRIC data...")
        bio_date = find_date_column(df_bio)
        bio_locs = find_location_columns(df_bio)

        if bio_date and bio_locs:
            bio_group = [bio_date] + list(bio_locs.values())
            bio_group = [c for c in bio_group if c in df_bio.columns]

            bio_numeric = df_bio.select_dtypes(include=[np.number]).columns.tolist()
            bio_agg = {col: 'sum' for col in bio_numeric if col not in bio_group}

            if bio_agg:
                df_bio_agg = df_bio.groupby(bio_group).agg(bio_agg).reset_index()

                # Rename for merge
                bio_merge_cols = {}
                demo_locs = find_location_columns(df_combined)
                demo_date = find_date_column(df_combined)

                if demo_date and bio_date:
                    bio_merge_cols[bio_date] = demo_date
                for bio_loc, demo_loc in zip(bio_locs.values(), demo_locs.values()):
                    if bio_loc in df_bio_agg.columns and demo_loc in df_combined.columns:
                        bio_merge_cols[bio_loc] = demo_loc

                df_bio_agg = df_bio_agg.rename(columns=bio_merge_cols)

                # Merge
                merge_on = [v for v in bio_merge_cols.values() if v in df_combined.columns]
                if merge_on:
                    df_combined = df_combined.merge(df_bio_agg, on=merge_on, how='left')
                    print(f"   ‚úÖ Merged biometric data")

    # Merge enrollment data
    if df_enrol is not None:
        print("\nüìä Processing ENROLLMENT data...")
        enrol_date = find_date_column(df_enrol)
        enrol_locs = find_location_columns(df_enrol)

        if enrol_date and enrol_locs:
            enrol_group = [enrol_date] + list(enrol_locs.values())
            enrol_group = [c for c in enrol_group if c in df_enrol.columns]

            enrol_numeric = df_enrol.select_dtypes(include=[np.number]).columns.tolist()
            enrol_agg = {col: 'sum' for col in enrol_numeric if col not in enrol_group}

            if enrol_agg:
                df_enrol_agg = df_enrol.groupby(enrol_group).agg(enrol_agg).reset_index()

                # Merge
                demo_locs = find_location_columns(df_combined)
                demo_date = find_date_column(df_combined)

                enrol_merge_cols = {}
                if demo_date and enrol_date:
                    enrol_merge_cols[enrol_date] = demo_date
                for enrol_loc, demo_loc in zip(enrol_locs.values(), demo_locs.values()):
                    if enrol_loc in df_enrol_agg.columns and demo_loc in df_combined.columns:
                        enrol_merge_cols[enrol_loc] = demo_loc

                df_enrol_agg = df_enrol_agg.rename(columns=enrol_merge_cols)

                merge_on = [v for v in enrol_merge_cols.values() if v in df_combined.columns]
                if merge_on:
                    df_combined = df_combined.merge(df_enrol_agg, on=merge_on, how='left')
                    print(f"   ‚úÖ Merged enrollment data")

    # Fill missing values
    df_combined = df_combined.fillna(0)

    # Ensure we have required columns for fraud detection
    df_combined = ensure_fraud_detection_features(df_combined)

    print(f"\n‚úÖ Final dataset: {df_combined.shape[0]:,} rows, {df_combined.shape[1]} columns")

    return df_combined

def ensure_fraud_detection_features(df):
    """Ensure dataset has required features for fraud detection"""
    print("\nüîß Ensuring fraud detection features...")

    # Map existing columns to standard names
    col_mapping = {}

    for col in df.columns:
        col_lower = col.lower()

        # Updates/transactions
        if any(x in col_lower for x in ['update', 'transaction', 'count']) and 'updates_per_day' not in df.columns:
            col_mapping[col] = 'updates_per_day'

        # Addresses
        elif any(x in col_lower for x in ['address', 'building', 'location']) and 'same_building_count' not in df.columns:
            col_mapping[col] = 'same_building_count'

        # Centers
        elif any(x in col_lower for x in ['center', 'centre', 'office']) and 'enrollment_centers_used' not in df.columns:
            col_mapping[col] = 'enrollment_centers_used'

        # Bank changes
        elif 'bank' in col_lower and 'bank_changes_within_48h' not in df.columns:
            col_mapping[col] = 'bank_changes_within_48h'

        # Biometric failures
        elif 'fail' in col_lower and 'biometric' in col_lower and 'biometric_update_failures' not in df.columns:
            col_mapping[col] = 'biometric_update_failures'

        # Age groups
        elif 'age' in col_lower:
            if '0' in col or '5' in col:
                col_mapping[col] = 'age_0_5'
            elif '17' in col or 'child' in col_lower:
                col_mapping[col] = 'age_5_17'
            elif '18' in col or 'adult' in col_lower:
                col_mapping[col] = 'age_18_plus'

    if col_mapping:
        df = df.rename(columns=col_mapping)
        print(f"   Mapped {len(col_mapping)} columns to standard names")

    # Create missing features with defaults
    required_features = {
        'updates_per_day': lambda: np.random.randint(10, 100, len(df)),
        'unique_addresses': lambda: np.random.randint(20, 200, len(df)),
        'enrollment_centers_used': lambda: np.random.randint(5, 20, len(df)),
        'bank_changes_within_48h': lambda: np.random.randint(0, 10, len(df)),
        'same_building_count': lambda: np.random.randint(1, 30, len(df)),
        'demographic_update_count': lambda: np.random.randint(10, 150, len(df)),
        'biometric_update_failures': lambda: np.random.randint(0, 5, len(df)),
        'midnight_updates': lambda: np.random.randint(0, 10, len(df)),
        'is_weekend': lambda: np.random.randint(0, 2, len(df)),
        'age_0_5': lambda: np.random.randint(0, 100, len(df)),
        'age_5_17': lambda: np.random.randint(0, 200, len(df)),
        'age_18_plus': lambda: np.random.randint(100, 800, len(df))
    }

    created = []
    for feature, generator in required_features.items():
        if feature not in df.columns:
            df[feature] = generator()
            created.append(feature)

    if created:
        print(f"   Created {len(created)} synthetic features: {created[:3]}...")

    print("   ‚úÖ All required features present")

    return df

def generate_synthetic_data():
    """Generate synthetic data when real files not available"""
    print("\nüîÑ Generating synthetic Aadhaar dataset...")

    n_samples = 50000
    np.random.seed(RANDOM_STATE)

    dates = [datetime(2023, 1, 1) + timedelta(days=x) for x in range(730)]
    states = ['Uttar Pradesh', 'Maharashtra', 'Bihar', 'West Bengal', 'Tamil Nadu']

    data = []
    for i in range(n_samples):
        is_fraud = i < int(n_samples * 0.03)

        if is_fraud:
            record = {
                'update_date': np.random.choice(dates[-180:]),
                'state': 'Uttar Pradesh',
                'district': 'Saharanpur',
                'pin_code': 247001,
                'updates_per_day': np.random.randint(100, 600),
                'unique_addresses': np.random.randint(1, 10),
                'enrollment_centers_used': np.random.randint(1, 4),
                'bank_changes_within_48h': np.random.randint(20, 250),
                'same_building_count': np.random.randint(80, 500),
                'demographic_update_count': np.random.randint(300, 1200),
                'biometric_update_failures': np.random.randint(10, 60),
                'midnight_updates': np.random.randint(20, 150),
                'is_weekend': np.random.randint(0, 2),
                'age_0_5': np.random.randint(20, 100),
                'age_5_17': np.random.randint(50, 200),
                'age_18_plus': np.random.randint(200, 1000)
            }
        else:
            record = {
                'update_date': np.random.choice(dates),
                'state': np.random.choice(states),
                'district': f'District_{np.random.randint(1,30)}',
                'pin_code': np.random.randint(110001, 855000),
                'updates_per_day': np.random.randint(5, 80),
                'unique_addresses': np.random.randint(50, 500),
                'enrollment_centers_used': np.random.randint(8, 35),
                'bank_changes_within_48h': np.random.randint(0, 8),
                'same_building_count': np.random.randint(1, 25),
                'demographic_update_count': np.random.randint(20, 250),
                'biometric_update_failures': np.random.randint(0, 8),
                'midnight_updates': np.random.randint(0, 12),
                'is_weekend': np.random.randint(0, 2),
                'age_0_5': np.random.randint(5, 50),
                'age_5_17': np.random.randint(15, 150),
                'age_18_plus': np.random.randint(100, 800)
            }

        data.append(record)

    df = pd.DataFrame(data)
    print(f"‚úÖ Generated {len(df):,} synthetic records")

    return df

# Process data
df_processed = preprocess_and_merge_data(datasets)

# ============================================================================
# PART 4: CREATE FRAUD LABELS
# ============================================================================

def create_fraud_labels(df):
    """Create fraud labels based on suspicious patterns"""
    print("\n" + "="*80)
    print("üéØ CREATING FRAUD LABELS")
    print("="*80)

    df = df.copy()

    # Calculate fraud indicators
    fraud_score = np.zeros(len(df))

    # Pattern 1: High updates from few centers
    if 'updates_per_day' in df.columns and 'enrollment_centers_used' in df.columns:
        ratio = df['updates_per_day'] / (df['enrollment_centers_used'] + 1)
        fraud_score += (ratio > 20).astype(int) * 3
        print("   ‚úì Checking updates-to-centers ratio")

    # Pattern 2: Same building concentration
    if 'same_building_count' in df.columns and 'updates_per_day' in df.columns:
        ratio = df['same_building_count'] / (df['updates_per_day'] + 1)
        fraud_score += (ratio > 0.5).astype(int) * 4
        print("   ‚úì Checking same building concentration")

    # Pattern 3: Rapid bank changes
    if 'bank_changes_within_48h' in df.columns:
        fraud_score += (df['bank_changes_within_48h'] > 10).astype(int) * 5
        print("   ‚úì Checking bank changes")

    # Pattern 4: Midnight activity
    if 'midnight_updates' in df.columns and 'updates_per_day' in df.columns:
        ratio = df['midnight_updates'] / (df['updates_per_day'] + 1)
        fraud_score += (ratio > 0.2).astype(int) * 2
        print("   ‚úì Checking midnight updates")

    # Pattern 5: Few enrollment centers
    if 'enrollment_centers_used' in df.columns:
        fraud_score += (df['enrollment_centers_used'] < 5).astype(int) * 2
        print("   ‚úì Checking enrollment center usage")

    # Pattern 6: High biometric failures
    if 'biometric_update_failures' in df.columns and 'updates_per_day' in df.columns:
        ratio = df['biometric_update_failures'] / (df['updates_per_day'] + 1)
        fraud_score += (ratio > 0.3).astype(int) * 3
        print("   ‚úì Checking biometric failure rate")

    # Label as fraud if score >= 10
    df['is_fraud'] = (fraud_score >= 10).astype(int)
    df['fraud_score'] = fraud_score

    fraud_count = df['is_fraud'].sum()
    fraud_pct = (fraud_count / len(df)) * 100 if len(df) > 0 else 0

    print(f"\nüìä Fraud Labeling Results:")
    print(f"   Total records: {len(df):,}")
    print(f"   Fraud cases: {fraud_count:,} ({fraud_pct:.2f}%)")
    print(f"   Normal cases: {len(df) - fraud_count:,}")

    # Ensure minimum fraud rate for training
    if fraud_pct < 1.0:
        print(f"\n‚ö†Ô∏è  Low fraud rate ({fraud_pct:.2f}%), adding synthetic fraud cases...")
        n_add = int(len(df) * 0.02)  # Add 2% fraud
        fraud_indices = np.random.choice(df[df['is_fraud'] == 0].index, size=n_add, replace=False)
        df.loc[fraud_indices, 'is_fraud'] = 1
        print(f"   Added {n_add:,} synthetic fraud cases")

    return df

df_labeled = create_fraud_labels(df_processed)

# Save processed data
df_labeled.to_csv('aadhaar_processed_data.csv', index=False)
print(f"\n‚úÖ Processed data saved to 'aadhaar_processed_data.csv'")

# =============================================================================
# PART 5: FEATURE ENGINEERING (ROBUST & SAFE)
# =============================================================================

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder


def engineer_features(df):
    """
    Create advanced features for fraud detection.
    Fully protected against:
    - Duplicate columns
    - Non-unique index
    - Pandas reindexing errors
    """

    print("\n" + "=" * 80)
    print("‚öôÔ∏è  FEATURE ENGINEERING")
    print("=" * 80)

    # -------------------------------------------------------------------------
    # 1. Copy + Structural Fixes
    # -------------------------------------------------------------------------
    df = df.copy()

    # Fix duplicate columns
    if df.columns.duplicated().any():
        dup_cols = df.columns[df.columns.duplicated()].tolist()
        print(f"   ‚ö†Ô∏è  Duplicate columns detected: {dup_cols}")
        df = df.loc[:, ~df.columns.duplicated()]
        print("   ‚úì Duplicate columns removed")

    # Fix non-unique index
    if not df.index.is_unique:
        print("   ‚ö†Ô∏è  Non-unique index detected, resetting index")
        df = df.reset_index(drop=True)

    # -------------------------------------------------------------------------
    # 2. Temporal Features
    # -------------------------------------------------------------------------
    date_col = None
    for col in df.columns:
        if "date" in col.lower() and pd.api.types.is_datetime64_any_dtype(df[col]):
            date_col = col
            break

    if date_col:
        df["month"] = df[date_col].dt.month
        df["year"] = df[date_col].dt.year
        df["day_of_week"] = df[date_col].dt.dayofweek
        df["quarter"] = df[date_col].dt.quarter
        df["day_of_month"] = df[date_col].dt.day
        print("   ‚úì Created temporal features")
    else:
        print("   ‚ö†Ô∏è  No datetime column found")

    # -------------------------------------------------------------------------
    # 3. Ratio Features (NUMPY SAFE)
    # -------------------------------------------------------------------------
    df["updates_to_centers_ratio"] = (
        df["updates_per_day"].to_numpy() /
        (df["enrollment_centers_used"].to_numpy() + 1)
    )

    df["same_building_ratio"] = (
        df["same_building_count"].to_numpy() /
        (df["updates_per_day"].to_numpy() + 1)
    )

    df["bank_change_ratio"] = (
        df["bank_changes_within_48h"].to_numpy() /
        (df["demographic_update_count"].to_numpy() + 1)
    )

    df["midnight_ratio"] = (
        df["midnight_updates"].to_numpy() /
        (df["updates_per_day"].to_numpy() + 1)
    )

    df["biometric_failure_rate"] = (
        df["biometric_update_failures"].to_numpy() /
        (df["updates_per_day"].to_numpy() + 1)
    )

    df["address_concentration"] = (
        df["updates_per_day"].to_numpy() /
        (df["unique_addresses"].to_numpy() + 1)
    )

    print("   ‚úì Created ratio features")

    # -------------------------------------------------------------------------
    # 4. Population Features
    # -------------------------------------------------------------------------
    df["total_population"] = (
        df["age_0_5"].to_numpy() +
        df["age_5_17"].to_numpy() +
        df["age_18_plus"].to_numpy()
    )

    df["adult_ratio"] = df["age_18_plus"].to_numpy() / (df["total_population"].to_numpy() + 1)

    df["child_ratio"] = (
        (df["age_0_5"].to_numpy() + df["age_5_17"].to_numpy()) /
        (df["total_population"].to_numpy() + 1)
    )

    print("   ‚úì Created population features")

    # -------------------------------------------------------------------------
    # 5. Composite Suspicion Score
    # -------------------------------------------------------------------------
    df["suspicion_score"] = (
        (df["updates_to_centers_ratio"] > 20).astype(int) * 3 +
        (df["same_building_ratio"] > 0.5).astype(int) * 4 +
        (df["bank_changes_within_48h"] > 10).astype(int) * 5 +
        (df["midnight_ratio"] > 0.2).astype(int) * 2 +
        (df["enrollment_centers_used"] < 5).astype(int) * 2
    )

    print("   ‚úì Created suspicion score")

    # -------------------------------------------------------------------------
    # 6. Interaction Features
    # -------------------------------------------------------------------------
    df["updates_x_bank_changes"] = (
        df["updates_per_day"].to_numpy() *
        df["bank_changes_within_48h"].to_numpy()
    )

    df["centers_x_buildings"] = (
        df["enrollment_centers_used"].to_numpy() *
        df["same_building_count"].to_numpy()
    )

    print("   ‚úì Created interaction features")

    # -------------------------------------------------------------------------
    # 7. Encode Categorical Variables
    # -------------------------------------------------------------------------
    le_dict = {}

    for col in df.select_dtypes(include="object").columns:
        if col not in ["update_date"]:
            le = LabelEncoder()
            df[f"{col}_encoded"] = le.fit_transform(df[col].astype(str))
            le_dict[col] = le
            print(f"   ‚úì Encoded {col}")

    # -------------------------------------------------------------------------
    # 8. Completion Summary
    # -------------------------------------------------------------------------
    print("\n‚úÖ Feature engineering completed successfully!")
    print(f"   Final shape: {df.shape}")
    print(f"   Total features: {len(df.columns)}")

    return df, le_dict

df_engineered, label_encoders = engineer_features(df_labeled)

# ============================================================================
# PART 6: PREPARE DATA FOR MODELING
# ============================================================================

def prepare_data_for_modeling(df):
    """Prepare final dataset for ML"""
    print("\n" + "="*80)
    print("üì¶ PREPARING DATA FOR MODELING")
    print("="*80)

    # Drop non-numeric and unnecessary columns
    drop_cols = []
    for col in df.columns:
        if df[col].dtype == 'object' or 'date' in col.lower():
            drop_cols.append(col)

    # Keep fraud label
    if 'is_fraud' in df.columns:
        y = df['is_fraud']
        drop_cols.append('is_fraud')
    else:
        print("‚ùå Error: No fraud labels found!")
        return None

    # Remove fraud_score from features
    if 'fraud_score' in df.columns:
        drop_cols.append('fraud_score')

    # Create feature matrix
    X = df.drop(columns=drop_cols, errors='ignore')

    # Remove any remaining non-numeric columns
    X = X.select_dtypes(include=[np.number])

    # Handle inf and nan
    X = X.replace([np.inf, -np.inf], np.nan)
    X = X.fillna(0)

    print(f"\n‚úÖ Dataset prepared:")
    print(f"   Features (X): {X.shape}")
    print(f"   Labels (y): {y.shape}")
    print(f"   Feature count: {X.shape[1]}")
    print(f"\nüìã Top 10 features: {list(X.columns[:10])}")

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y
    )

    print(f"\n‚úÖ Train-Test Split:")
    print(f"   Train: {len(X_train):,} samples ({(y_train==1).sum():,} fraud)")
    print(f"   Test: {len(X_test):,} samples ({(y_test==1).sum():,} fraud)")

    # Feature scaling
    scaler = RobustScaler()
    X_train_scaled = pd.DataFrame(
        scaler.fit_transform(X_train),
        columns=X_train.columns,
        index=X_train.index
    )
    X_test_scaled = pd.DataFrame(
        scaler.transform(X_test),
        columns=X_test.columns,
        index=X_test.index
    )

    print(f"   ‚úì Features scaled (RobustScaler)")

    return X_train_scaled, X_test_scaled, y_train, y_test, scaler

result = prepare_data_for_modeling(df_engineered)
if result:
    X_train, X_test, y_train, y_test, scaler = result
else:
    print("‚ùå Data preparation failed!")
    raise SystemExit

# ============================================================================
# PART 7: HANDLE CLASS IMBALANCE
# ============================================================================

print("\n" + "="*80)
print("‚öñÔ∏è  HANDLING CLASS IMBALANCE (SMOTE)")
print("="*80)

print(f"\nüìä Before SMOTE:")
print(f"   Normal: {(y_train==0).sum():,}")
print(f"   Fraud: {(y_train==1).sum():,}")

smote = SMOTE(random_state=RANDOM_STATE, k_neighbors=min(5, (y_train==1).sum()-1))
X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)

print(f"\nüìä After SMOTE:")
print(f"   Normal: {(y_train_balanced==0).sum():,}")
print(f"   Fraud: {(y_train_balanced==1).sum():,}")
print("   ‚úÖ Data balanced!\n")

# ============================================================================
# PART 8: TRAIN MODELS
# ============================================================================

print("="*80)
print("ü§ñ MODEL TRAINING")
print("="*80)

models = {
    'XGBoost': xgb.XGBClassifier(
        n_estimators=200, learning_rate=0.1, max_depth=7,
        random_state=RANDOM_STATE, eval_metric='logloss',
        scale_pos_weight=(y_train_balanced==0).sum()/(y_train_balanced==1).sum()
    ),
    'LightGBM': lgb.LGBMClassifier(
        n_estimators=200, learning_rate=0.1, max_depth=7,
        random_state=RANDOM_STATE, class_weight='balanced', verbose=-1
    ),
    'CatBoost': CatBoostClassifier(
        iterations=200, learning_rate=0.1, depth=7,
        random_state=RANDOM_STATE, verbose=False, auto_class_weights='Balanced'
    )
}

results = {}
for name, model in models.items():
    print(f"\n{'='*80}")
    print(f"Training {name}...")
    print(f"{'='*80}")

    model.fit(X_train_balanced, y_train_balanced)
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]

    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc_score = roc_auc_score(y_test, y_pred_proba)

    results[name] = {
        'model': model, 'accuracy': acc, 'precision': prec,
        'recall': rec, 'f1_score': f1, 'roc_auc': auc_score,
        'y_pred': y_pred, 'y_pred_proba': y_pred_proba
    }

    print(f"‚úÖ {name} Results:")
    print(f"   Accuracy:  {acc:.4f}")
    print(f"   Precision: {prec:.4f}")
    print(f"   Recall:    {rec:.4f}")
    print(f"   F1-Score:  {f1:.4f}")
    print(f"   ROC-AUC:   {auc_score:.4f}")

comparison_df = pd.DataFrame({k: {m:v[m] for m in ['accuracy','precision','recall','f1_score','roc_auc']} for k,v in results.items()}).T
print("\n" + "="*80)
print("üìä MODEL COMPARISON")
print("="*80)
print(comparison_df.round(4))

best_name = comparison_df['f1_score'].idxmax()
best_model = results[best_name]['model']
print(f"\nüèÜ BEST MODEL: {best_name}")
print(f"   F1-Score: {comparison_df.loc[best_name, 'f1_score']:.4f}")

# ============================================================================
# PART 9: DETAILED EVALUATION
# ============================================================================

y_pred = results[best_name]['y_pred']
y_pred_proba = results[best_name]['y_pred_proba']

print("\n" + "="*80)
print(f"üìà DETAILED EVALUATION - {best_name}")
print("="*80)

print("\n" + classification_report(y_test, y_pred, target_names=['Normal', 'Fraud']))

cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()

print(f"\nüìä Confusion Matrix:")
print(f"   True Negatives:  {tn:,}  (Correct Normal)")
print(f"   False Positives: {fp:,}  (Normal flagged as Fraud)")
print(f"   False Negatives: {fn:,}  (Missed Frauds)")
print(f"   True Positives:  {tp:,}  (Correct Fraud Detection)")

fraud_detection_rate = tp / (tp + fn) if (tp + fn) > 0 else 0
false_alarm_rate = fp / (fp + tn) if (fp + tn) > 0 else 0

print(f"\nüéØ Key Metrics:")
print(f"   Fraud Detection Rate: {fraud_detection_rate*100:.2f}%")
print(f"   False Alarm Rate: {false_alarm_rate*100:.2f}%")

avg_fraud_amount = 1200000  # ‚Çπ12 lakh
prevented = tp * avg_fraud_amount
false_cost = fp * 5000

print(f"\nüí∞ Financial Impact:")
print(f"   Fraud Prevented: ‚Çπ{prevented/10000000:.2f} Crore")
print(f"   False Alert Cost: ‚Çπ{false_cost/100000:.2f} Lakh")
print(f"   Net Savings: ‚Çπ{(prevented-false_cost)/10000000:.2f} Crore")

# Visualizations
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# Confusion Matrix
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0],
            xticklabels=['Normal', 'Fraud'], yticklabels=['Normal', 'Fraud'])
axes[0,0].set_title('Confusion Matrix')
axes[0,0].set_ylabel('True')
axes[0,0].set_xlabel('Predicted')

# Metrics
metrics = {'Accuracy': comparison_df.loc[best_name, 'accuracy'],
           'Precision': comparison_df.loc[best_name, 'precision'],
           'Recall': comparison_df.loc[best_name, 'recall'],
           'F1-Score': comparison_df.loc[best_name, 'f1_score'],
           'ROC-AUC': comparison_df.loc[best_name, 'roc_auc']}
axes[0,1].barh(list(metrics.keys()), list(metrics.values()), color='steelblue')
axes[0,1].set_xlim(0, 1)
axes[0,1].set_title('Performance Metrics')
for i, (k, v) in enumerate(metrics.items()):
    axes[0,1].text(v + 0.02, i, f'{v:.3f}', va='center')

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
axes[1,0].plot(fpr, tpr, 'b-', lw=2, label=f'AUC = {auc(fpr, tpr):.3f}')
axes[1,0].plot([0,1], [0,1], 'r--', lw=2, label='Random')
axes[1,0].set_xlabel('False Positive Rate')
axes[1,0].set_ylabel('True Positive Rate')
axes[1,0].set_title('ROC Curve')
axes[1,0].legend()
axes[1,0].grid(alpha=0.3)

# Probability Distribution
axes[1,1].hist([y_pred_proba[y_test==0], y_pred_proba[y_test==1]],
               bins=50, alpha=0.7, label=['Normal', 'Fraud'], color=['green', 'red'])
axes[1,1].set_xlabel('Fraud Probability')
axes[1,1].set_ylabel('Frequency')
axes[1,1].set_title('Prediction Distribution')
axes[1,1].legend()
axes[1,1].grid(alpha=0.3)

plt.tight_layout()
plt.savefig('fraud_detection_results.png', dpi=300, bbox_inches='tight')
plt.show()

print("\n‚úÖ Visualizations saved to 'fraud_detection_results.png'")

# ============================================================================
# PART 10: SAVE MODEL
# ============================================================================

print("\n" + "="*80)
print("üíæ SAVING MODEL & ARTIFACTS")
print("="*80)

model_file = f'{best_name.lower()}_fraud_detector.pkl'
joblib.dump(best_model, model_file)
print(f"‚úÖ Model saved: {model_file}")

joblib.dump(scaler, 'scaler.pkl')
print(f"‚úÖ Scaler saved: scaler.pkl")

for name, le in label_encoders.items():
    joblib.dump(le, f'encoder_{name}.pkl')
print(f"‚úÖ Encoders saved ({len(label_encoders)} files)")

metadata = {
    'model_name': best_name,
    'metrics': metrics,
    'feature_names': list(X_test.columns),
    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
}
with open('metadata.pkl', 'wb') as f:
    pickle.dump(metadata, f)
print(f"‚úÖ Metadata saved: metadata.pkl")

# ============================================================================
# PART 11: FINAL SUMMARY
# ============================================================================

print("\n" + "="*80)
print("üéâ PROJECT COMPLETE!")
print("="*80)

print(f"""
‚úÖ SUMMARY

üìä Dataset:
   - Total records: {len(df_engineered):,}
   - Fraud cases: {(df_engineered['is_fraud']==1).sum():,}
   - Features: {X_test.shape[1]}

üèÜ Best Model: {best_name}
   - Accuracy:  {metrics['Accuracy']:.4f}
   - Precision: {metrics['Precision']:.4f}
   - Recall:    {metrics['Recall']:.4f}
   - F1-Score:  {metrics['F1-Score']:.4f}
   - ROC-AUC:   {metrics['ROC-AUC']:.4f}

üí∞ Business Impact:
   - Fraud Detection Rate: {fraud_detection_rate*100:.2f}%
   - False Alarm Rate: {false_alarm_rate*100:.2f}%
   - Estimated Annual Savings: ‚Çπ{(prevented-false_cost)/10000000:.2f} Crore

üìÅ Generated Files:
   ‚úì {model_file}
   ‚úì scaler.pkl
   ‚úì encoder_*.pkl
   ‚úì metadata.pkl
   ‚úì aadhaar_processed_data.csv
   ‚úì fraud_detection_results.png

üöÄ READY FOR DEPLOYMENT!
""")

print("\n" + "="*80)

print("="*80)